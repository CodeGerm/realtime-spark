spark {
  master = "yarn-client" # important
  #master = ${?SPARK_MASTER}

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4

  jobserver {
    port = 8090
    jobdao = spark.jobserver.io.JobSqlDAO

    sqldao {
      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = /database

      # Full JDBC URL / init string.  Sorry, needs to match above.
      # Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass
      jdbc.url = "jdbc:h2:file:/database/h2-db"
    }

    job-jar-paths {    # NOTE: you may need an absolute path below
      channel = /opt/mount/host/work/github/spark-databroker/target/org.cg.spark-databroker-0.0.1-SNAPSHOT.jar
    }

    context-per-jvm = true

    context-creation-timeout = 120 s
    yarn-context-creation-timeout = 120 s
  }

  # predefined Spark contexts
  # contexts {
  #   my-low-latency-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #   }
  #   # define additional contexts here
  # }

  # contexts {
  #   stream-context {
  #     num-cpu-cores = 1           # Number of cores to allocate.  Required.
  #     memory-per-node = 1G         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
  #     context-factory = spark.jobserver.context.StreamingContextFactory
  #     streaming.batch_interval = 6
  #     streaming.stopGracefully = true
  #     streaming.stopSparkContext = true
  #   }
     # define additional contexts here
  # }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    # choose a port that is free on your system and also the 16 (depends on max retries for submitting the job) next portnumbers should be free

    spark.driver.port = 32456
    #spark.driver.host = Jobserver
    #spark.local.ip = Jobserver

    # defines the place where your spark-assembly jar is located in your hdfs
    spark.master = "yarn-client"
    spark.yarn.jar = "hdfs://hadoop:9000/spark_jars/spark-assembly-1.5.2-hadoop2.6.0.jar"

    num-cpu-cores = 1            # Number of cores to allocate.  Required.
    memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
    context-init-timeout = 120 s

    # in case spark distribution should be accessed from HDFS (as opposed to being installed on every mesos slave)
    # spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"

    # uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','
    # dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]
      dependent-jar-uris = ["file:///opt/mount/host/work/github/spark-databroker/target/org.cg.spark-databroker-0.0.1-SNAPSHOT.jar","file:///opt/mount/host/Documents/centrify/git/platform/com.centrify.platform.databroker/target/com.centrify.platform.databroker-0.0.1-SNAPSHOT-jar-with-dependencies.jar"]
    # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
    # such as hadoop connection settings that don't use the "spark." prefix
    passthrough {
      #es.nodes = "192.1.1.1"
    }
  }

  # This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
  home = "/spark"
}
deploy {
    manager-start-cmd = "/app/manager_start.sh"
}


spray.can.server {
  idle-timeout = 130 s
  request-timeout = 120 s
  # Increase this in order to upload bigger job jars
  parsing.max-content-length = 180m
}
